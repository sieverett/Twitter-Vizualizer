{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import tweepy\n",
    "import time\n",
    "import config_twit\n",
    "import jsonpickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import csv\n",
    "from geopy.geocoders import Nominatim\n",
    "geolocator = Nominatim(user_agent=\"specify_your_app_name_here\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "query = '#kathua OR \"kathua\" AND \"rape\"'\n",
    "max_tweets = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Enter your keys/secrets as strings in the following fields. config_twit is a .py file that contains \n",
    "# my twitter api auth codes \n",
    "\n",
    "API_KEY = config_twit.api_key\n",
    "API_SECRET = config_twit.api_secret\n",
    "ACCESS_TOKEN = config_twit.ACCESS_TOKEN\n",
    "ACCESS_TOKEN_SECRET = config_twit.ACCESS_TOKEN_SECRET\n",
    "\n",
    "# assuming twitter_authentication.py contains each of the 4 oauth elements (1 per line)\n",
    "\n",
    "auth = tweepy.OAuthHandler(API_KEY, API_SECRET)\n",
    "auth.set_access_token(ACCESS_TOKEN, ACCESS_TOKEN_SECRET)\n",
    "\n",
    "api = tweepy.API(auth)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# extract data from tweets that match search terms \n",
    "\n",
    "searched_tweets = [status for status in tweepy.Cursor(api.search, q=query).items(max_tweets)]\n",
    "tweet_id = [s.id for s in searched_tweets]\n",
    "user_id = [s.user.id for s in searched_tweets]\n",
    "screen_name = [s.user.screen_name for s in searched_tweets]\n",
    "followers = [s.user.followers_count for s in searched_tweets]\n",
    "retweet_count = [s.retweet_count for s in searched_tweets]\n",
    "locations = [s.user.location for s in searched_tweets]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%time\n",
    "node:\n",
    "{\"name\":\"Mme.Burgon\",\"group\":7}\n",
    "\n",
    "link:\n",
    "{\"source\":1,\"target\":0,\"value\":1}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df to parse data into network graph - here make df for convenience but later could have lists\n",
    "\n",
    "df = pd.DataFrame({'tweet_id':tweet_id, 'screen_name':screen_name, 'source':user_id, 'followers':followers, 'locations':locations})\n",
    "most_followed_tweeters = df.sort_values(by='followers', ascending=False).head(10).drop_duplicates(subset='screen_name')\n",
    "most_followed_tweets = zip(most_followed_tweeters.tweet_id, most_followed_tweeters.screen_name, \n",
    "                           most_followed_tweeters.followers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Links list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make links list: get retweet list of each tweet\n",
    "\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True, compression=True)\n",
    "\n",
    "users_retweeted_dict ={}\n",
    "\n",
    "retweet_lst_names=[]\n",
    "for status_id, user_id, followers in most_followed_tweets:\n",
    "    \n",
    "    retweet_lst=[]\n",
    "    retweet_lst_name =[]\n",
    "    \n",
    "    for retweet_id in api.retweets(status_id):\n",
    "        retweet_lst.append(retweet_id.user.id)\n",
    "        retweet_lst_name.append(retweet_id.user.name)\n",
    "        \n",
    "    \n",
    "    users_retweeted_dict[user_id] = retweet_lst_name\n",
    "    retweet_lst_names.append(retweet_lst_name)\n",
    "\n",
    "# flatten list\n",
    "retweet_lst_names = [j for i in retweet_lst_names for j in i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for j,k,i in most_followed_tweets:\n",
    "    print(j,k,i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# then replace tweet id with index number\n",
    "\n",
    "links_lst=[]\n",
    "for k in users_retweeted_dict:\n",
    "\n",
    "    if users_retweeted_dict[k]:\n",
    "        for u in users_retweeted_dict[k]:\n",
    "            links_lst.append({\"source\":k,\"target\":u})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get followers counts\n",
    "\n",
    "top_tweeters_follower_counts=[]\n",
    "mft = most_followed_tweeters.followers\n",
    "\n",
    "normalized_followers = np.round((np.abs(np.array(mft)-np.array(mft).mean())/mft.std())*10, 0)\n",
    "\n",
    "\n",
    "\n",
    "count=0\n",
    "\n",
    "for i in links_lst:\n",
    "    for j,k in zip(most_followed_tweeters.source, normalized_followers ) :\n",
    "        \n",
    "        if i['source'] == j:\n",
    "            \n",
    "            top_tweeters_follower_counts.append(int(k))\n",
    "    \n",
    "    \n",
    "# for k,v in zip(most_followed_tweeters.user_id, most_followed_tweeters.followers):\n",
    "    \n",
    "    \n",
    "#    most_followed_tweeters.followers\n",
    "# links_lst[0]['source']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# put all elements in link list together\n",
    "\n",
    "count=0\n",
    "count1=0\n",
    "links_r =[]\n",
    "for k,v in users_retweeted_dict.items():\n",
    "    for y,t in zip(most_followed_tweeters.source, normalized_followers ):\n",
    "        if y == k:\n",
    "            value = t\n",
    "    for i in v:\n",
    "        links_r.append({\"source\":k, \"target\": i, \"value\": int(value)})\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Node list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# node: make \"group\" from location\n",
    "\n",
    "location_india =[]\n",
    "for i in most_followed_tweeters.locations:\n",
    "    if 'India' or 'New Delhi' in i:\n",
    "        location_india.append(1)\n",
    "    else:\n",
    "        location_india.append(0)\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make nodes lst\n",
    "\n",
    "nodes_lst = []\n",
    "for x,y in zip(most_followed_tweeters.screen_name, location_india):\n",
    "     nodes_lst.append({\"id\":x, \"group\":y})\n",
    "\n",
    "for x,y in enumerate(retweet_lst_names):\n",
    "     nodes_lst.append({\"id\":y, \"group\":2})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# putting it together\n",
    "data = {}\n",
    "data['nodes'] = nodes_lst\n",
    "data['links'] = links_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "with open('network_data_twitter.json', 'w') as outfile:\n",
    "    json.dump(data, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# build the dataframe\n",
    "\n",
    "followers = [s.user.followers_count for s in searched_tweets]\n",
    "friends = [s.user.friends_count for s in searched_tweets]\n",
    "language = [s.lang for s in searched_tweets]\n",
    "listed_count = [s.user.listed_count for s in searched_tweets]\n",
    "locations = [s.user.location for s in searched_tweets]\n",
    "messages = [s.text.encode('utf8') for s in searched_tweets]\n",
    "posted_date = [s.created_at for s in searched_tweets]\n",
    "retweet_count = [s.retweet_count for s in searched_tweets]\n",
    "screen_name = [s.user.screen_name for s in searched_tweets]\n",
    "tweet_id = [s.id for s in searched_tweets]\n",
    "tweet_source = [s.source for s in searched_tweets] \n",
    "user_created_at = [s.user.created_at for s in searched_tweets]\n",
    "user_id = [s.user.id for s in searched_tweets]\n",
    "user_name = [s.user.name for s in searched_tweets]\n",
    "geo = [s.geo for s in searched_tweets]\n",
    "\n",
    "df = pd.DataFrame({\n",
    "'followers': followers,\n",
    "'friends': friends,\n",
    "'language' : language,\n",
    "'listed_count' : listed_count,\n",
    "'locations' : locations,\n",
    "'messages' : messages,\n",
    "'posted_date' : posted_date,\n",
    "'retweet_count': retweet_count,\n",
    "'screen_name' : screen_name,\n",
    "'tweet_id': tweet_id,\n",
    "'tweet_source': tweet_source, \n",
    "'user_created_at': user_created_at,\n",
    "'user_id': user_id,\n",
    "'user_name': user_name,\n",
    "\n",
    "})\n",
    "\n",
    "# output to json file for downloading\n",
    "df.to_csv('general_data_twitter.csv')\n",
    "# print(df.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Map data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a small_lst for demonstration purposes only\n",
    "\n",
    "small_lst = df[:100]\n",
    "location_small_lst, message_small_lst = small_lst.locations,small_lst.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# makes api call to translate location name into coordinates based on location field \n",
    "\n",
    "coordinates=[]\n",
    "for index,loc in enumerate(location_small_lst):\n",
    "    try:\n",
    "        location = geolocator.geocode(loc)\n",
    "        coordinates.append({\"lat\":location.latitude, \"lng\":location.longitude, \"title\":message_small_lst[index]})\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('map_data.json', 'w') as the_file:\n",
    "    for item in coordinates:\n",
    "        the_file.write(\"{}\".format(item))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Timeseries brush data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# rolling sum over data for each hour\n",
    "\n",
    "df_roll = df.set_index(\"posted_date\")\n",
    "df_roll = df_roll.resample(\"1h\").sum().fillna(0).rolling(window=3, min_periods=1).mean()\n",
    "df_roll.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# output timeseries data for brush\n",
    "\n",
    "data_roll=[]\n",
    "data_retweet=[]\n",
    "for i,j in enumerate(df_roll['posted_date']):\n",
    "    data_roll.append(j.strftime(\"%Y %b %d %H %M\")) \n",
    "    data_retweet.append(round(df.retweet_count[i],0))\n",
    "brush_df = pd.DataFrame({'date':data_roll,'price':data_retweet})\n",
    "brush_df.to_csv('brush_data.csv')\n",
    "\n",
    "with open('brush_data.csv','w') as myfile:\n",
    "    wr = csv.writer(myfile, quoting=csv.QUOTE_ALL)\n",
    "    wr.writerow(data_brush)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> gauge data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gauge = df.set_index(\"posted_date\")\n",
    "df_gauge = df_gauge.resample(\"1d\").sum().fillna(0).rolling(window=3, min_periods=1).mean()\n",
    "df_gauge.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gauge.to_csv('gauge_data.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
