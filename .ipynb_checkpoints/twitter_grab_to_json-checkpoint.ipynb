{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import tweepy\n",
    "import time\n",
    "import config_twit\n",
    "import jsonpickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from geopy.geocoders import Nominatim\n",
    "geolocator = Nominatim(user_agent=\"specify_your_app_name_here\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "query = '#kathua OR \"kathua\" AND \"rape\"'\n",
    "max_tweets = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Enter your keys/secrets as strings in the following fields. config_twit is a .py file that contains \n",
    "# my twitter api auth codes \n",
    "\n",
    "API_KEY = config_twit.api_key\n",
    "API_SECRET = config_twit.api_secret\n",
    "ACCESS_TOKEN = config_twit.ACCESS_TOKEN\n",
    "ACCESS_TOKEN_SECRET = config_twit.ACCESS_TOKEN_SECRET\n",
    "\n",
    "# assuming twitter_authentication.py contains each of the 4 oauth elements (1 per line)\n",
    "\n",
    "auth = tweepy.OAuthHandler(API_KEY, API_SECRET)\n",
    "auth.set_access_token(ACCESS_TOKEN, ACCESS_TOKEN_SECRET)\n",
    "\n",
    "api = tweepy.API(auth)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# extract data from tweets that match search terms \n",
    "\n",
    "searched_tweets = [status for status in tweepy.Cursor(api.search, q=query).items(max_tweets)]\n",
    "tweet_id = [s.id for s in searched_tweets]\n",
    "user_id = [s.user.id for s in searched_tweets]\n",
    "screen_name = [s.user.screen_name for s in searched_tweets]\n",
    "followers = [s.user.followers_count for s in searched_tweets]\n",
    "retweet_count = [s.retweet_count for s in searched_tweets]\n",
    "locations = [s.user.location for s in searched_tweets]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%time\n",
    "node:\n",
    "{\"name\":\"Mme.Burgon\",\"group\":7}\n",
    "\n",
    "link:\n",
    "{\"source\":1,\"target\":0,\"value\":1}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df to parse data into network graph - here make df for convenience but later could have lists\n",
    "\n",
    "screen_name = [s.user.screen_name for s in searched_tweets]\n",
    "df = pd.DataFrame({'tweet_id':tweet_id, 'screen_name':screen_name, 'source':user_id, 'followers':followers, 'locations':locations})\n",
    "most_followed_tweeters = df.sort_values(by='followers', ascending=False).head(10).drop_duplicates(subset='screen_name')\n",
    "most_followed_tweets = zip(most_followed_tweeters.tweet_id, most_followed_tweeters.source, \n",
    "                           most_followed_tweeters.followers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Links list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make links list: get retweet list of each tweet\n",
    "\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True, compression=True)\n",
    "\n",
    "users_retweeted_dict ={}\n",
    "\n",
    "retweet_lst_names=[]\n",
    "for status_id, user_id, followers in most_followed_tweets:\n",
    "    \n",
    "    retweet_lst=[]\n",
    "    retweet_lst_name =[]\n",
    "    \n",
    "    for retweet_id in api.retweets(status_id):\n",
    "        retweet_lst.append(retweet_id.user.id)\n",
    "        retweet_lst_name.append(retweet_id.user.name)\n",
    "\n",
    "    users_retweeted_dict[user_id] = retweet_lst\n",
    "    retweet_lst_names.append(retweet_lst_name)\n",
    "\n",
    "# flatten list\n",
    "retweet_lst_names = [j for i in retweet_lst_names for j in i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# then replace tweet id with index number\n",
    "\n",
    "links_lst=[]\n",
    "for k in users_retweeted_dict:\n",
    "\n",
    "    if users_retweeted_dict[k]:\n",
    "        for u in users_retweeted_dict[k]:\n",
    "            links_lst.append({\"source\":k,\"target\":u})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get followers counts\n",
    "\n",
    "top_tweeters_follower_counts=[]\n",
    "mft = most_followed_tweeters.followers\n",
    "\n",
    "normalized_followers = np.round((np.abs(np.array(mft)-np.array(mft).mean())/mft.std())*10, 0)\n",
    "\n",
    "\n",
    "\n",
    "count=0\n",
    "\n",
    "for i in links_lst:\n",
    "    for j,k in zip(most_followed_tweeters.source, normalized_followers ) :\n",
    "        \n",
    "        if i['source'] == j:\n",
    "            \n",
    "            top_tweeters_follower_counts.append(int(k))\n",
    "    \n",
    "    \n",
    "# for k,v in zip(most_followed_tweeters.user_id, most_followed_tweeters.followers):\n",
    "    \n",
    "    \n",
    "#    most_followed_tweeters.followers\n",
    "# links_lst[0]['source']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# put all elements in link list together\n",
    "\n",
    "count=0\n",
    "count1=0\n",
    "links_r =[]\n",
    "for k,v in users_retweeted_dict.items():\n",
    "    count1+=1\n",
    "    for y,t in zip(most_followed_tweeters.source, normalized_followers ):\n",
    "        if y == k:\n",
    "            value = t\n",
    "    for i in v:\n",
    "        count+=1\n",
    "        links_r.append({\"source\":count1, \"target\": count, \"value\": int(value)})\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Node list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# node: make \"group\" from location\n",
    "\n",
    "location_india =[]\n",
    "for i in most_followed_tweeters.locations:\n",
    "    if 'India' or 'New Delhi' in i:\n",
    "        location_india.append(1)\n",
    "    else:\n",
    "        location_india.append(0)\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make nodes lst\n",
    "\n",
    "nodes_lst = []\n",
    "for x,y in zip(most_followed_tweeters.screen_name, location_india):\n",
    "     nodes_lst.append({\"name\":x, \"group\":y})\n",
    "\n",
    "for x,y in enumerate(retweet_lst_names):\n",
    "     nodes_lst.append({\"name\":y, \"group\":2})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# putting it together\n",
    "data = {}\n",
    "data['nodes'] = nodes_lst\n",
    "data['links'] = links_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "with open('network_data_twitter.json', 'w') as outfile:\n",
    "    json.dump(data, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# build the dataframe\n",
    "\n",
    "followers = [s.user.followers_count for s in searched_tweets]\n",
    "friends = [s.user.friends_count for s in searched_tweets]\n",
    "language = [s.lang for s in searched_tweets]\n",
    "listed_count = [s.user.listed_count for s in searched_tweets]\n",
    "locations = [s.user.location for s in searched_tweets]\n",
    "messages = [s.text.encode('utf8') for s in searched_tweets]\n",
    "posted_date = [s.created_at for s in searched_tweets]\n",
    "retweet_count = [s.retweet_count for s in searched_tweets]\n",
    "screen_name = [s.user.screen_name for s in searched_tweets]\n",
    "tweet_id = [s.id for s in searched_tweets]\n",
    "tweet_source = [s.source for s in searched_tweets] \n",
    "user_created_at = [s.user.created_at for s in searched_tweets]\n",
    "user_id = [s.user.id for s in searched_tweets]\n",
    "user_name = [s.user.name for s in searched_tweets]\n",
    "geo = [s.geo for s in searched_tweets]\n",
    "\n",
    "df = pd.DataFrame({\n",
    "'followers': followers,\n",
    "'friends': friends,\n",
    "'language' : language,\n",
    "'listed_count' : listed_count,\n",
    "'locations' : locations,\n",
    "'messages' : messages,\n",
    "'posted_date' : posted_date,\n",
    "'retweet_count': retweet_count,\n",
    "'screen_name' : screen_name,\n",
    "'tweet_id': tweet_id,\n",
    "'tweet_source': tweet_source, \n",
    "'user_created_at': user_created_at,\n",
    "'user_id': user_id,\n",
    "'user_name': user_name,\n",
    "\n",
    "})\n",
    "\n",
    "# output to json file for downloading\n",
    "df.to_csv('general_data_twitter.csv')\n",
    "# print(df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_lst = df[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_small_lst, message_small_lst = small_lst.locations,small_lst.messages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "coordinates=[]\n",
    "for index,loc in enumerate(location_small_lst):\n",
    "    try:\n",
    "        location = geolocator.geocode(loc)\n",
    "        coordinates.append({\"lat\":location.latitude, \"lng\":location.longitude, \"title\":message_small_lst[index]})\n",
    "    except:\n",
    "        pass\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.set_index(\"posted_date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.resample(\"1h\").sum().fillna(0).rolling(window=3, min_periods=1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.groupby(df.index.map(lambda t: t.minute)).mean()\n",
    "\n",
    "\n",
    "data_brush=[]\n",
    "for i,j in enumerate(df['posted_date']):\n",
    "    data_brush.append('{}, {}'.format(j.strftime(\"%Y %b %d %H %M\"), round(df.retweet_count[i],0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('brush_data.json', 'w') as outfile:\n",
    "    json.dump(data_brush, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2018 Jul 28 13 00, 3765.0',\n",
       " '2018 Jul 28 14 00, 1882.0',\n",
       " '2018 Jul 28 15 00, 1255.0',\n",
       " '2018 Jul 28 16 00, 0.0',\n",
       " '2018 Jul 28 17 00, 0.0',\n",
       " '2018 Jul 28 18 00, 0.0',\n",
       " '2018 Jul 28 19 00, 335.0',\n",
       " '2018 Jul 28 20 00, 335.0',\n",
       " '2018 Jul 28 21 00, 335.0',\n",
       " '2018 Jul 28 22 00, 0.0',\n",
       " '2018 Jul 28 23 00, 0.0',\n",
       " '2018 Jul 29 00 00, 0.0',\n",
       " '2018 Jul 29 01 00, 0.0',\n",
       " '2018 Jul 29 02 00, 0.0',\n",
       " '2018 Jul 29 03 00, 4.0',\n",
       " '2018 Jul 29 04 00, 16.0',\n",
       " '2018 Jul 29 05 00, 59.0',\n",
       " '2018 Jul 29 06 00, 55.0',\n",
       " '2018 Jul 29 07 00, 304.0',\n",
       " '2018 Jul 29 08 00, 261.0',\n",
       " '2018 Jul 29 09 00, 265.0',\n",
       " '2018 Jul 29 10 00, 4.0',\n",
       " '2018 Jul 29 11 00, 12.0',\n",
       " '2018 Jul 29 12 00, 435.0',\n",
       " '2018 Jul 29 13 00, 487.0',\n",
       " '2018 Jul 29 14 00, 522.0',\n",
       " '2018 Jul 29 15 00, 116.0',\n",
       " '2018 Jul 29 16 00, 68.0',\n",
       " '2018 Jul 29 17 00, 25.0',\n",
       " '2018 Jul 29 18 00, 29.0',\n",
       " '2018 Jul 29 19 00, 25.0',\n",
       " '2018 Jul 29 20 00, 25.0',\n",
       " '2018 Jul 29 21 00, 1.0',\n",
       " '2018 Jul 29 22 00, 0.0',\n",
       " '2018 Jul 29 23 00, 0.0',\n",
       " '2018 Jul 30 00 00, 0.0',\n",
       " '2018 Jul 30 01 00, 4.0',\n",
       " '2018 Jul 30 02 00, 4.0',\n",
       " '2018 Jul 30 03 00, 4.0',\n",
       " '2018 Jul 30 04 00, 32.0',\n",
       " '2018 Jul 30 05 00, 48.0',\n",
       " '2018 Jul 30 06 00, 72.0',\n",
       " '2018 Jul 30 07 00, 88.0',\n",
       " '2018 Jul 30 08 00, 411.0',\n",
       " '2018 Jul 30 09 00, 755.0',\n",
       " '2018 Jul 30 10 00, 1046.0',\n",
       " '2018 Jul 30 11 00, 713.0',\n",
       " '2018 Jul 30 12 00, 5614.0',\n",
       " '2018 Jul 30 13 00, 18346.0',\n",
       " '2018 Jul 30 14 00, 26332.0',\n",
       " '2018 Jul 30 15 00, 25182.0',\n",
       " '2018 Jul 30 16 00, 15203.0',\n",
       " '2018 Jul 30 17 00, 9515.0',\n",
       " '2018 Jul 30 18 00, 7287.0',\n",
       " '2018 Jul 30 19 00, 6293.0',\n",
       " '2018 Jul 30 20 00, 4180.0',\n",
       " '2018 Jul 30 21 00, 2695.0',\n",
       " '2018 Jul 30 22 00, 875.0',\n",
       " '2018 Jul 30 23 00, 936.0',\n",
       " '2018 Jul 31 00 00, 988.0',\n",
       " '2018 Jul 31 01 00, 1260.0',\n",
       " '2018 Jul 31 02 00, 1448.0',\n",
       " '2018 Jul 31 03 00, 1877.0',\n",
       " '2018 Jul 31 04 00, 2759.0',\n",
       " '2018 Jul 31 05 00, 2921.0',\n",
       " '2018 Jul 31 06 00, 3122.0',\n",
       " '2018 Jul 31 07 00, 2323.0',\n",
       " '2018 Jul 31 08 00, 2131.0',\n",
       " '2018 Jul 31 09 00, 1555.0',\n",
       " '2018 Jul 31 10 00, 1225.0',\n",
       " '2018 Jul 31 11 00, 1567.0',\n",
       " '2018 Jul 31 12 00, 1227.0',\n",
       " '2018 Jul 31 13 00, 1069.0',\n",
       " '2018 Jul 31 14 00, 470.0',\n",
       " '2018 Jul 31 15 00, 683.0',\n",
       " '2018 Jul 31 16 00, 802.0',\n",
       " '2018 Jul 31 17 00, 647.0',\n",
       " '2018 Jul 31 18 00, 265.0',\n",
       " '2018 Jul 31 19 00, 37.0',\n",
       " '2018 Jul 31 20 00, 37.0',\n",
       " '2018 Jul 31 21 00, 35.0',\n",
       " '2018 Jul 31 22 00, 10.0',\n",
       " '2018 Jul 31 23 00, 18.0',\n",
       " '2018 Aug 01 00 00, 148.0',\n",
       " '2018 Aug 01 01 00, 143.0',\n",
       " '2018 Aug 01 02 00, 463.0',\n",
       " '2018 Aug 01 03 00, 334.0',\n",
       " '2018 Aug 01 04 00, 645.0',\n",
       " '2018 Aug 01 05 00, 1170.0',\n",
       " '2018 Aug 01 06 00, 1264.0',\n",
       " '2018 Aug 01 07 00, 1048.0',\n",
       " '2018 Aug 01 08 00, 448.0',\n",
       " '2018 Aug 01 09 00, 445.0',\n",
       " '2018 Aug 01 10 00, 345.0',\n",
       " '2018 Aug 01 11 00, 170.0',\n",
       " '2018 Aug 01 12 00, 2119.0',\n",
       " '2018 Aug 01 13 00, 2346.0',\n",
       " '2018 Aug 01 14 00, 2443.0',\n",
       " '2018 Aug 01 15 00, 2941.0',\n",
       " '2018 Aug 01 16 00, 4175.0',\n",
       " '2018 Aug 01 17 00, 8730.0',\n",
       " '2018 Aug 01 18 00, 13353.0',\n",
       " '2018 Aug 01 19 00, 15656.0',\n",
       " '2018 Aug 01 20 00, 12564.0',\n",
       " '2018 Aug 01 21 00, 6710.0']"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_brush"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pd.read_csv('twitter_data_new.csv',encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import re\n",
    "re.sub(\"[^0-9]\", \"\", df.locations[0])\n",
    "\n",
    "if not re.search('\\d+', value):\n",
    "    # no numbers\n",
    "else:\n",
    "    # numbers present\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "location_nums=[]\n",
    "location_other=[]\n",
    "for i,j in enumerate(df.locations):\n",
    "    if re.sub(\"[^0-9]\", \"\", j) != \"\":\n",
    "        location_nums.append((i,j))\n",
    "    else:\n",
    "        location_other.append((i,j))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gmplot import gmplot\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML('<h1>Hello, world!</h1>'))\n",
    "\n",
    "\n",
    "gmap = gmplot.GoogleMapPlotter(37.766956, -122.438481, 13)\n",
    "# gmap = gmplot.GoogleMapPlotter.from_geocode(\"San Francisco\")\n",
    "\n",
    "top_attraction_lats, top_attraction_lons = zip(*[\n",
    "    (37.769901, -122.498331),\n",
    "    (37.768645, -122.475328),\n",
    "    (37.771478, -122.468677),\n",
    "    (37.769867, -122.466102),\n",
    "    (37.767187, -122.467496),\n",
    "    (37.770104, -122.470436)\n",
    "    ])\n",
    "\n",
    "gmap.scatter(top_attraction_lats, top_attraction_lons, '#3B0B39', size=40, marker=False\n",
    "             \n",
    "gmap.draw(\"my_map.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
